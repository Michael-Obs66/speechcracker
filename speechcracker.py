# -*- coding: utf-8 -*-
"""speechcracker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XArG_zLV3rcnUWOju2CaNpD2qD_cG8ac
"""

!pip install spacy flask flask-restful flask-cors pandas numpy scikit-learn matplotlib seaborn
!python -m spacy download en_core_web_sm

import os

os.makedirs("app", exist_ok=True)

open("app/nlp.py", "w").close()
open("app/insights.py", "w").close()

open("main.py", "w").close()

with open("sample_text.txt", "w") as f:
    f.write("Today, we are gathered here to discuss the future of our economy. "
            "Innovation, technology, and education are key components of our strategy.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.py
# from flask import Flask, request, jsonify
# from flask_cors import CORS
# from app.nlp import analyze_speech
# from app.insights import generate_insights
# 
# app = Flask(__name__)
# CORS(app)
# 
# @app.route("/analyze", methods=["POST"])
# def analyze():
#     content = request.json.get("text", "")
#     if not content:
#         return jsonify({"error": "No text provided"}), 400
# 
#     analysis = analyze_speech(content)
#     insights = generate_insights(analysis)
#     return jsonify({"analysis": analysis, "insights": insights})
# 
# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=5000)
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app/nlp.py
# import spacy
# from collections import Counter
# 
# nlp = spacy.load("en_core_web_sm")
# 
# def analyze_speech(text):
#     doc = nlp(text)
#     tokens = [(token.text, token.pos_, token.dep_) for token in doc]
#     word_count = Counter([token.text.lower() for token in doc if token.is_alpha])
# 
#     categorized = {
#         "subjects": [chunk.text for chunk in doc.noun_chunks],
#         "verbs": [token.text for token in doc if token.pos_ == "VERB"],
#         "objects": [token.text for token in doc if token.dep_ == "dobj"],
#         "adverbs": [token.text for token in doc if token.pos_ == "ADV"]
#     }
# 
#     return {"tokens": tokens, "word_count": word_count, "categorized": categorized}
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app/insights.py
# def generate_insights(analysis):
#     word_count = analysis["word_count"]
#     top_words = word_count.most_common(5)
#     subjects = analysis["categorized"]["subjects"]
# 
#     insights = {
#         "most_frequent_words": top_words,
#         "key_subjects": subjects[:3]
#     }
#     return insights
#

!python main.py

!pip install pyngrok

from pyngrok import ngrok

# Jalankan Flask di background
!python main.py &

# Tampilkan URL publik
public_url = ngrok.connect(5000)
print(f"Access your app at {public_url}")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile sample_text.txt
# Today, we are gathered here to discuss the future of our economy. Innovation, technology, and education are key components of our strategy.
#

import requests

url = "<ngrok_public_url>/analyze"
with open("sample_text.txt", "r") as file:
    text = file.read()

response = requests.post(url, json={"text": text})
print(response.json())

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# Ambil hasil analisis
word_count = response.json()["analysis"]["word_count"]

# Visualisasi frekuensi kata
words, counts = zip(*word_count.items())
plt.figure(figsize=(10, 6))
sns.barplot(x=list(counts)[:10], y=list(words)[:10])
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.title("Top 10 Words")
plt.show()